{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaunak Chadha\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland2.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  5468\n",
      "Total Vocab:  34\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  5368\n"
     ]
    }
   ],
   "source": [
    "\t\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "        seq_in = raw_text[i:i + seq_length]\n",
    "        seq_out = raw_text[i + seq_length]\n",
    "        dataX.append([char_to_int[char] for char in seq_in])\n",
    "        dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5368/5368 [==============================] - 42s 8ms/step - loss: 3.1236\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.12359, saving model to weights-improvement-01-3.1236.hdf5\n",
      "Epoch 2/50\n",
      "5368/5368 [==============================] - 43s 8ms/step - loss: 3.0251\n",
      "\n",
      "Epoch 00002: loss improved from 3.12359 to 3.02505, saving model to weights-improvement-02-3.0251.hdf5\n",
      "Epoch 3/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 3.0153\n",
      "\n",
      "Epoch 00003: loss improved from 3.02505 to 3.01533, saving model to weights-improvement-03-3.0153.hdf5\n",
      "Epoch 4/50\n",
      "5368/5368 [==============================] - 41s 8ms/step - loss: 3.0157\n",
      "\n",
      "Epoch 00004: loss did not improve\n",
      "Epoch 5/50\n",
      "5368/5368 [==============================] - 45s 8ms/step - loss: 3.0091\n",
      "\n",
      "Epoch 00005: loss improved from 3.01533 to 3.00906, saving model to weights-improvement-05-3.0091.hdf5\n",
      "Epoch 6/50\n",
      "5368/5368 [==============================] - 45s 8ms/step - loss: 3.0055\n",
      "\n",
      "Epoch 00006: loss improved from 3.00906 to 3.00549, saving model to weights-improvement-06-3.0055.hdf5\n",
      "Epoch 7/50\n",
      "5368/5368 [==============================] - 42s 8ms/step - loss: 2.9938\n",
      "\n",
      "Epoch 00007: loss improved from 3.00549 to 2.99378, saving model to weights-improvement-07-2.9938.hdf5\n",
      "Epoch 8/50\n",
      "5368/5368 [==============================] - 41s 8ms/step - loss: 2.9683\n",
      "\n",
      "Epoch 00008: loss improved from 2.99378 to 2.96834, saving model to weights-improvement-08-2.9683.hdf5\n",
      "Epoch 9/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 2.9227\n",
      "\n",
      "Epoch 00009: loss improved from 2.96834 to 2.92270, saving model to weights-improvement-09-2.9227.hdf5\n",
      "Epoch 10/50\n",
      "5368/5368 [==============================] - 42s 8ms/step - loss: 2.8538\n",
      "\n",
      "Epoch 00010: loss improved from 2.92270 to 2.85377, saving model to weights-improvement-10-2.8538.hdf5\n",
      "Epoch 11/50\n",
      "5368/5368 [==============================] - 43s 8ms/step - loss: 2.7860\n",
      "\n",
      "Epoch 00011: loss improved from 2.85377 to 2.78603, saving model to weights-improvement-11-2.7860.hdf5\n",
      "Epoch 12/50\n",
      "5368/5368 [==============================] - 44s 8ms/step - loss: 2.7249\n",
      "\n",
      "Epoch 00012: loss improved from 2.78603 to 2.72494, saving model to weights-improvement-12-2.7249.hdf5\n",
      "Epoch 13/50\n",
      "5368/5368 [==============================] - 40s 8ms/step - loss: 2.6560\n",
      "\n",
      "Epoch 00013: loss improved from 2.72494 to 2.65597, saving model to weights-improvement-13-2.6560.hdf5\n",
      "Epoch 14/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 2.5786\n",
      "\n",
      "Epoch 00014: loss improved from 2.65597 to 2.57855, saving model to weights-improvement-14-2.5786.hdf5\n",
      "Epoch 15/50\n",
      "5368/5368 [==============================] - 39s 7ms/step - loss: 2.5070\n",
      "\n",
      "Epoch 00015: loss improved from 2.57855 to 2.50704, saving model to weights-improvement-15-2.5070.hdf5\n",
      "Epoch 16/50\n",
      "5368/5368 [==============================] - 39s 7ms/step - loss: 2.4310\n",
      "\n",
      "Epoch 00016: loss improved from 2.50704 to 2.43100, saving model to weights-improvement-16-2.4310.hdf5\n",
      "Epoch 17/50\n",
      "5368/5368 [==============================] - 39s 7ms/step - loss: 2.3740\n",
      "\n",
      "Epoch 00017: loss improved from 2.43100 to 2.37397, saving model to weights-improvement-17-2.3740.hdf5\n",
      "Epoch 18/50\n",
      "5368/5368 [==============================] - 39s 7ms/step - loss: 2.3356\n",
      "\n",
      "Epoch 00018: loss improved from 2.37397 to 2.33563, saving model to weights-improvement-18-2.3356.hdf5\n",
      "Epoch 19/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 2.3041\n",
      "\n",
      "Epoch 00019: loss improved from 2.33563 to 2.30409, saving model to weights-improvement-19-2.3041.hdf5\n",
      "Epoch 20/50\n",
      "5368/5368 [==============================] - 39s 7ms/step - loss: 2.2284\n",
      "\n",
      "Epoch 00020: loss improved from 2.30409 to 2.22838, saving model to weights-improvement-20-2.2284.hdf5\n",
      "Epoch 21/50\n",
      "5368/5368 [==============================] - 39s 7ms/step - loss: 2.1649\n",
      "\n",
      "Epoch 00021: loss improved from 2.22838 to 2.16493, saving model to weights-improvement-21-2.1649.hdf5\n",
      "Epoch 22/50\n",
      "5368/5368 [==============================] - 39s 7ms/step - loss: 2.1002\n",
      "\n",
      "Epoch 00022: loss improved from 2.16493 to 2.10020, saving model to weights-improvement-22-2.1002.hdf5\n",
      "Epoch 23/50\n",
      "5368/5368 [==============================] - 39s 7ms/step - loss: 2.0288\n",
      "\n",
      "Epoch 00023: loss improved from 2.10020 to 2.02877, saving model to weights-improvement-23-2.0288.hdf5\n",
      "Epoch 24/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 1.9767\n",
      "\n",
      "Epoch 00024: loss improved from 2.02877 to 1.97674, saving model to weights-improvement-24-1.9767.hdf5\n",
      "Epoch 25/50\n",
      "5368/5368 [==============================] - 44s 8ms/step - loss: 1.8808\n",
      "\n",
      "Epoch 00025: loss improved from 1.97674 to 1.88081, saving model to weights-improvement-25-1.8808.hdf5\n",
      "Epoch 26/50\n",
      "5368/5368 [==============================] - 41s 8ms/step - loss: 1.7912\n",
      "\n",
      "Epoch 00026: loss improved from 1.88081 to 1.79119, saving model to weights-improvement-26-1.7912.hdf5\n",
      "Epoch 27/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 1.7101\n",
      "\n",
      "Epoch 00027: loss improved from 1.79119 to 1.71014, saving model to weights-improvement-27-1.7101.hdf5\n",
      "Epoch 28/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 1.6311\n",
      "\n",
      "Epoch 00028: loss improved from 1.71014 to 1.63106, saving model to weights-improvement-28-1.6311.hdf5\n",
      "Epoch 29/50\n",
      "5368/5368 [==============================] - 39s 7ms/step - loss: 1.5433\n",
      "\n",
      "Epoch 00029: loss improved from 1.63106 to 1.54329, saving model to weights-improvement-29-1.5433.hdf5\n",
      "Epoch 30/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 1.4460\n",
      "\n",
      "Epoch 00030: loss improved from 1.54329 to 1.44601, saving model to weights-improvement-30-1.4460.hdf5\n",
      "Epoch 31/50\n",
      "5368/5368 [==============================] - 41s 8ms/step - loss: 1.3635\n",
      "\n",
      "Epoch 00031: loss improved from 1.44601 to 1.36348, saving model to weights-improvement-31-1.3635.hdf5\n",
      "Epoch 32/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 1.2744\n",
      "\n",
      "Epoch 00032: loss improved from 1.36348 to 1.27436, saving model to weights-improvement-32-1.2744.hdf5\n",
      "Epoch 33/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 1.2115\n",
      "\n",
      "Epoch 00033: loss improved from 1.27436 to 1.21149, saving model to weights-improvement-33-1.2115.hdf5\n",
      "Epoch 34/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 1.1348\n",
      "\n",
      "Epoch 00034: loss improved from 1.21149 to 1.13484, saving model to weights-improvement-34-1.1348.hdf5\n",
      "Epoch 35/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 1.3206\n",
      "\n",
      "Epoch 00035: loss did not improve\n",
      "Epoch 36/50\n",
      "5368/5368 [==============================] - 45s 8ms/step - loss: 1.5675\n",
      "\n",
      "Epoch 00036: loss did not improve\n",
      "Epoch 37/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 1.1493\n",
      "\n",
      "Epoch 00037: loss did not improve\n",
      "Epoch 38/50\n",
      "5368/5368 [==============================] - 41s 8ms/step - loss: 0.9929\n",
      "\n",
      "Epoch 00038: loss improved from 1.13484 to 0.99286, saving model to weights-improvement-38-0.9929.hdf5\n",
      "Epoch 39/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 0.9144\n",
      "\n",
      "Epoch 00039: loss improved from 0.99286 to 0.91438, saving model to weights-improvement-39-0.9144.hdf5\n",
      "Epoch 40/50\n",
      "5368/5368 [==============================] - 42s 8ms/step - loss: 0.8542\n",
      "\n",
      "Epoch 00040: loss improved from 0.91438 to 0.85424, saving model to weights-improvement-40-0.8542.hdf5\n",
      "Epoch 41/50\n",
      "5368/5368 [==============================] - 42s 8ms/step - loss: 0.8022\n",
      "\n",
      "Epoch 00041: loss improved from 0.85424 to 0.80222, saving model to weights-improvement-41-0.8022.hdf5\n",
      "Epoch 42/50\n",
      "5368/5368 [==============================] - 46s 8ms/step - loss: 0.7459\n",
      "\n",
      "Epoch 00042: loss improved from 0.80222 to 0.74593, saving model to weights-improvement-42-0.7459.hdf5\n",
      "Epoch 43/50\n",
      "5368/5368 [==============================] - 43s 8ms/step - loss: 0.6841\n",
      "\n",
      "Epoch 00043: loss improved from 0.74593 to 0.68411, saving model to weights-improvement-43-0.6841.hdf5\n",
      "Epoch 44/50\n",
      "5368/5368 [==============================] - 41s 8ms/step - loss: 0.6424\n",
      "\n",
      "Epoch 00044: loss improved from 0.68411 to 0.64238, saving model to weights-improvement-44-0.6424.hdf5\n",
      "Epoch 45/50\n",
      "5368/5368 [==============================] - 41s 8ms/step - loss: 0.6072\n",
      "\n",
      "Epoch 00045: loss improved from 0.64238 to 0.60723, saving model to weights-improvement-45-0.6072.hdf5\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5368/5368 [==============================] - 42s 8ms/step - loss: 0.5653\n",
      "\n",
      "Epoch 00046: loss improved from 0.60723 to 0.56532, saving model to weights-improvement-46-0.5653.hdf5\n",
      "Epoch 47/50\n",
      "5368/5368 [==============================] - 40s 8ms/step - loss: 0.5190\n",
      "\n",
      "Epoch 00047: loss improved from 0.56532 to 0.51900, saving model to weights-improvement-47-0.5190.hdf5\n",
      "Epoch 48/50\n",
      "5368/5368 [==============================] - 45s 8ms/step - loss: 0.4883\n",
      "\n",
      "Epoch 00048: loss improved from 0.51900 to 0.48829, saving model to weights-improvement-48-0.4883.hdf5\n",
      "Epoch 49/50\n",
      "5368/5368 [==============================] - 40s 7ms/step - loss: 0.4822\n",
      "\n",
      "Epoch 00049: loss improved from 0.48829 to 0.48221, saving model to weights-improvement-49-0.4822.hdf5\n",
      "Epoch 50/50\n",
      "5368/5368 [==============================] - 42s 8ms/step - loss: 0.4554\n",
      "\n",
      "Epoch 00050: loss improved from 0.48221 to 0.45536, saving model to weights-improvement-50-0.4554.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24e9320d4e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights#\n",
    "filename = \"weights-improvement-50-0.4554.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" the door\n",
      "always looking up at higher floors\n",
      "want to see it all give me more (rise, rise up)\n",
      "i was al \"\n",
      "----------------------------------------------------------\n",
      "ways up for the making changes\n",
      "walking down the street and meeting strangers\n",
      "flipping through my lif\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#Random Seed Selection \n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "print(\"----------------------------------------------------------\")\n",
    "for i in range(100):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print( \"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"test_file.txt\", \"r\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Hello World”“This is our new text file”“and this is another line.”“Why? Because we can.”\n"
     ]
    }
   ],
   "source": [
    "print(file.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
